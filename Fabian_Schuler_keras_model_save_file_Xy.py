import numpy as np
from keras import layers, models, optimizers, backend
import tensorflow as tf
import random
import glob,csv

def main():
    #one less than in file bec. starts by zero
    category_start=[1,1542,4020,6962,7437,7479,8230,8628,9856]
    #c0 as splits(308*4+310):
    category_start_c0_1=[1,308]
    category_start_c0_2=[308,616]
    category_start_c0_3=[616,924]
    category_start_c0_4=[924,1232]
    category_start_c0_5=[1232,1542]
    #(21*103+3*105=3*826)=len(c1)
    category_start_c1_1=[1,1542,1645]
    category_start_c1_2=[1,1645,1748]
    category_start_c1_3=[1,1748,1851]
    category_start_c1_4=[1,1851,1954]
    category_start_c1_5=[1,1954,2057]
    category_start_c1_6=[1,2057,2160]
    category_start_c1_7=[1,2160,2263]
    category_start_c1_8=[1,2263,2366]
    ######p1 end####
    category_start_c1_9=[1,2366,2469]
    category_start_c1_10=[1,2469,2572]
    category_start_c1_11=[1,2572,2675]
    category_start_c1_12=[1,2675,2778]
    category_start_c1_13=[1,2778,2881]
    category_start_c1_14=[1,2881,2984]
    category_start_c1_15=[1,2984,3087]
    category_start_c1_16=[1,3087,3190]
    ####p2 end####
    category_start_c1_17=[1,3190,3293]
    category_start_c1_18=[1,3293,3396]
    category_start_c1_19=[1,3396,3499]
    category_start_c1_20=[1,3499,3602]
    category_start_c1_21=[1,3602,3707]
    category_start_c1_22=[1,3707,3812]
    category_start_c1_23=[1,3812,3917]
    category_start_c1_24=[1,3917,4020]
    ###p3 end####
    #980+980+982=len(c2)
    category_start_c2_1=[1,1542,4020,4265]
    category_start_c2_2=[1,1542,4265,4510]
    category_start_c2_3=[1,1542,4510,4755]
    category_start_c2_4=[1,1542,4755,5000]
    category_start_c2_5=[1,1542,5000,5245]
    category_start_c2_6=[1,1542,5245,5490]
    category_start_c2_7=[1,1542,5490,5735]
    category_start_c2_8=[1,1542,5735,5980]
    category_start_c2_9=[1,1542,5980,6225]
    category_start_c2_10=[1,1542,6225,6470]
    category_start_c2_11=[1,1542,6470,6715]
    category_start_c2_12=[1,1542,6715,6962]
    ##this is just for writing stuff in files##
    category_start_c0=[
        category_start_c0_1,
        category_start_c0_2,
        category_start_c0_3,
        category_start_c0_4,
        category_start_c0_5
    ]
    category_start_c1=[
        category_start_c1_1,
        category_start_c1_2,
        category_start_c1_3,
        category_start_c1_4,
        category_start_c1_5,
        category_start_c1_6,
        category_start_c1_7,
        category_start_c1_8,
        category_start_c1_9,
        category_start_c1_10,
        category_start_c1_11,
        category_start_c1_12,
        category_start_c1_13,
        category_start_c1_14,
        category_start_c1_15,
        category_start_c1_16,
        category_start_c1_17,
        category_start_c1_18,
        category_start_c1_19,
        category_start_c1_20,
        category_start_c1_21,
        category_start_c1_22,
        category_start_c1_23,
        category_start_c1_24       
        ]
    category_start_c2=[
        category_start_c2_1,
        category_start_c2_2,
        category_start_c2_3,
        category_start_c2_4,
        category_start_c2_5,
        category_start_c2_6,
        category_start_c2_7,
        category_start_c2_8,
        category_start_c2_9,
        category_start_c2_10,
        category_start_c2_11,
        category_start_c2_12
    ]
    category_start_subset=5
    category_start=category_start_c0[category_start_subset-1]
    category_name="1_"+str(category_start_subset)
    category_end=[]
    for i in range(len(category_start)-1):
        category_end.append(category_start[i+1])
    category_end.append(10868)
    category_len=1500
    category_included=[0,1,2,3,4,5,6,7,8]
    epochs=200

   
    file_content=[]
    file_category=[]
    #find the files that are the right entries in trainLabel.csv
    with open("src/trainLabels.csv",newline="") as csvfile:
        csvreader= csv.reader(csvfile)
        x=0
        range_included=[]
        range_category=[]
        for i in category_included:
            for j in range(category_start[i],min(category_start[i]+category_len,category_end[i])):
                range_included.append(j)
                range_category.append(i)

        for row in csvreader:            
            if x in range_included:
                file_content.append("../Kaggle_Datensatz/malware-classification/train/"+row[0]+".bytes")
            x+=1
        
        file_category=range_category

        print(len(file_content)," Train-Files used")
        print(len(file_category)," Train Labels used")
        

    
    
    #returns list with all files from file_content cleaned up
    def getMWFiles():
        print("get files start")
        l_loop=file_content
        l_ret=[]
        #reads all files and removes certain unimportant characters
        for i in l_loop:
            file_line=open(i, "r").readlines()
            file_cleaned=[]
            #remove address lines and newlines and exclude ??,FF,00,
            #because they are pretty common among all types and add nothing valuable
            for j in file_line:
                file_line_test=[x for x in j.rstrip("\n").split(" ")[1:] if not(x in ["??","00","FF"])]
                file_cleaned.append(file_line_test)
            l_ret.append(file_cleaned)
        print("get files end")
        return l_ret

    #finds for each file the correct Label in trainLabel.csv,
    #returns them and a List of all different Classes
    def getMWTypes():
        print("get class_labels start")
        l=[]
        diff_labels=[]
        reference=file_content
        #removes the "../Kaggle_Datensatz/malware-classification/train/" and ".bytes"
        #before and after the filename
        for i in range(len(reference)):
            #removes ".bytes"
            reference[i]=reference[i][:len(reference[i])-6]
            #removes "../Kaggle_Datensatz/malware-classification/train/"
            reference[i]=reference[i][49:]

        #reads all lines of trainlabels.csv and gets the labels of the files in reference
        with open("src/trainLabels.csv",newline="") as csvfile:
            csvreader= csv.reader(csvfile)
            for row in csvreader:
                #only works if used order == order from trainLabel.csv
                if row[1] in "123456789" and row[0] in reference:
                    v=int(row[1])
                    #converts Classes [1,2]-->[0,1]
                    v-=1
                    l.append(v)
                    if not(v in diff_labels):
                        diff_labels.append(v)
                        print("new label type",v,"added")

        print("get class_labels end")
        return l,diff_labels

    #generates the "alphabet" that is used (=all 256 Combinations + "??")
    def getAlphabet():
        l_4bit=["0","1","2","3","4","5","6","7","8","9","A","B","C","D","E","F"]
        l_Byte=["??"]
        for i in l_4bit:
            for j in l_4bit:
                l_Byte.append(i+j)
        return l_Byte
        


    #unimportant for preprocessing, needed later
    def ngram_block(n,alphabet_size):
        def wrapped(inputs):
            layer=layers.Conv1D(1,n,use_bias=False,trainable=False)
            x=layers.Reshape((-1,1))(inputs)
            x=layer(x)
            kernel=np.power(alphabet_size,range(0,n),dtype=backend.floatx())
            layer.set_weights([kernel.reshape(n,1,1)])
            return layers.Reshape((-1,))(x)
        return wrapped

    #mw_files contains the files=byte-files
    mw_files = getMWFiles()
    #class_labels contains the labels=classes, CLASS_COUNT is amount of Classes
    class_labels,CLASS_COUNT = getMWTypes()
    CLASS_COUNT=len(CLASS_COUNT)
    # needed to load data in small portions with only one class at a time, but still have all 9Classes in y
    if CLASS_COUNT==1:
        CLASS_COUNT=9



    
    print("len of mw_files:",len(mw_files))
    print("len of class_labels:",len(class_labels))
    print("amount of class_labels:",CLASS_COUNT)
    
    
    ALPHABET = getAlphabet()
    #trim sequence of indices to MAX_LEN
    MAX_LEN = 10000

    #encode characters by their index in ALPHABET
    def encode_sample(sample,index):
        indices=[]
        for line in sample:
            for char in line:
                indices.append(index[char])
        return np.resize(np.array(indices),MAX_LEN)

    #create indices for each possible Byte
    index={char: i for i, char in enumerate(ALPHABET)}
   
    #create data matrix X
    X=np.stack([encode_sample(x,index) for x in mw_files])
    #one-hot encoding
    y=np.eye(CLASS_COUNT)[class_labels]

    ################### write files in here #######################

    if category_name:
        np.save("new_test_2021_01_11/Xy/fileX_T"+category_name+".npy",X)
        np.save("new_test_2021_01_11/Xy/fileY_T"+category_name+".npy",y)    
    else:
        np.save("new_test_2021_01_11/Xy/fileX_T"+str(category_included[0]+1)+".npy",X)
        np.save("new_test_2021_01_11/Xy/fileY_T"+str(category_included[0]+1)+".npy",y)

    return

    X=np.load("new_test_2021_01_11/Xy/fileX.npy",allow_pickle=True)
    y=np.load("new_test_2021_01_11/Xy/fileY.npy",allow_pickle=True)

    return
    






















    #######(un)important stuff that gets implemented in other files#####


    rndm_indices=np.arange(X.shape[0])
    np.random.shuffle(rndm_indices)
    X=X[rndm_indices]
    y=y[rndm_indices]


    LAYER_PARAMS = [[64, 3, 3], [128, 3, 3]]
    EMBEDDING_DIM = 16

    #test stuff below(works now, but eats all my pc-power yummy):
    LAYER_PARAMS=[[2000,16,16]]
    EMBEDDING_DIM=32

    def build_ngram_model(n):
        inputs = layers.Input(shape=(MAX_LEN,))
        x = ngram_block(n, len(ALPHABET))(inputs)
        x = layers.Embedding(pow(len(ALPHABET), n), n * EMBEDDING_DIM)(x)
        for filters, kernel_size, pool_size in LAYER_PARAMS:
            x = layers.Conv1D(filters, kernel_size, activation="relu")(x)
            x = layers.BatchNormalization()(x)
            x = layers.SpatialDropout1D(0.05+0.1*n)(x)
            x = layers.MaxPooling1D(pool_size)(x)
        x = layers.GlobalAveragePooling1D()(x)
        x = layers.Dense(CLASS_COUNT, activation="softmax")(x)
        model = models.Model(inputs=inputs, outputs=x)
        
        model.compile(optimizer=optimizers.Adadelta(),
                    loss="categorical_crossentropy", metrics=["acc"])
        return model

    #loops for 1,2- and 3-grams over the model(grams with n>2 use A LOT of Memory!)
    for n in range(1, 2):
        print("################### Start Run with n=",n,"########################")
        build_ngram_model(n).fit(X, y, epochs=epochs, 
                             batch_size=1, validation_split=0.2)
        print("################### End Run with n=",n,"  ########################")


if __name__=="__main__":
    main()