import numpy as np
from keras import layers, models, optimizers, backend
import tensorflow as tf
import random
import glob,csv

def main():
    #with size=200: 55 batches needed
    batch_number=55
    batch_size=200
    file_amount=10873

    the_files=glob.glob("../Kaggle_Datensatz/malware-classification/test/*.bytes")



    print(the_files[0])
    file_content=the_files[(batch_number-1)*batch_size:min(batch_number*batch_size,file_amount)]
    print("First File(1batch):",file_content[0])
    print("Last File(1batch:",file_content[len(file_content)-1])
    print("len(batch):",len(file_content))

    #return
    
    
    #returns list with all files from the_files_used cleaned up
    def getMWFiles():
        print("get files start")
        l_loop=file_content
        l_ret=[]
        
        #reads all files and removes certain unimportant characters
        for i in l_loop:
            file_line=open(i, "r").readlines()
            file_cleaned=[]
            #remove address lines and newlines
            for j in file_line:
                file_line_test=[x for x in j.rstrip("\n").split(" ")[1:] if not(x in ["??","00","FF"])]
                file_cleaned.append(file_line_test)
            l_ret.append(file_cleaned)
        
        print("get files end")
        return l_ret

    

    #generates the "alphabet" that is used (=all 256 Combinations + "??")
    def getAlphabet():
        l_4bit=["0","1","2","3","4","5","6","7","8","9","A","B","C","D","E","F"]
        #s="??"
        l_Byte=["??"]
        for i in l_4bit:
            for j in l_4bit:
                #s+=i+j
                l_Byte.append(i+j)
        return l_Byte
        


    #posts contains the files
    posts = getMWFiles() 
    
    print("len of posts:",len(posts))
   
    
    
    ALPHABET = getAlphabet()
    #trim sequence of indices to MAX_LEN
    MAX_LEN = 10000

    #encode characters by their index in ALPHABET
    def encode_sample(sample,index):
        indices=[]
        for line in sample:
            for char in line:
                indices.append(index[char])
        return np.resize(np.array(indices),MAX_LEN)
    
    


    index={char: i for i, char in enumerate(ALPHABET)}

    X=np.stack([encode_sample(x,index) for x in posts])
   

    y=[x[48:len(x)-6] for x in file_content]

    np.save("new_test_2021_01_11/testData/X/fileX_"+str(batch_size*(batch_number-1)+1)+"_"+str(batch_size*batch_number)+".npy",X)
    np.save("new_test_2021_01_11/testData/Names/fileNames_"+str(batch_size*(batch_number-1)+1)+"_"+str(batch_size*batch_number)+".npy",y)

    print("batch "+str(batch_number)+" saved")

    return


if __name__=="__main__":
    main()