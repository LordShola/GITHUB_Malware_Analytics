import numpy as np
from keras import layers, models, optimizers, backend
import tensorflow as tf
import random
import glob,csv

def main():
    epochs=100
    TOPIC_COUNT=9
    loadModelFromDrive=True
    saveModelToDrive=True
    model_load_name="my_model_balanced_over90percentV6"
    model_save_name="my_model_balanced_over90percentV6"



    #################### Actual loading ############        

    X=np.load("new_test_2021_01_11/Xy/fileX_nd_balanced.npy",allow_pickle=True)
    y=np.load("new_test_2021_01_11/Xy/fileY_nd_balanced.npy",allow_pickle=True)
   
    rndm_indices=np.arange(X.shape[0])
    np.random.shuffle(rndm_indices)
    X=X[rndm_indices]
    y=y[rndm_indices]


    ################### stuff loaded #################

    #generates the "alphabet" that is used (=all 256 Combinations + "??")
    def getAlphabet():
        l_4bit=["0","1","2","3","4","5","6","7","8","9","A","B","C","D","E","F"]
        l_Byte=["??"]
        for i in l_4bit:
            for j in l_4bit:
                l_Byte.append(i+j)
        return l_Byte
        



    def ngram_block(n,alphabet_size):
        def wrapped(inputs):
            layer=layers.Conv1D(1,n,use_bias=False,trainable=False)
            x=layers.Reshape((-1,1))(inputs)
            x=layer(x)
            kernel=np.power(alphabet_size,range(0,n),dtype=backend.floatx())
            layer.set_weights([kernel.reshape(n,1,1)])
            return layers.Reshape((-1,))(x)
        return wrapped

    
    
    
    ALPHABET = getAlphabet()
    #trim sequence of indices to MAX_LEN
    MAX_LEN = 10000

    






    LAYER_PARAMS = [[64, 3, 3], [128, 3, 3]]
    EMBEDDING_DIM = 16

    #test stuff below(works now, but eats all my pc-power yummy):
    LAYER_PARAMS=[[2000,16,16]]
    EMBEDDING_DIM=32

    def build_ngram_model(n):
        inputs = layers.Input(shape=(MAX_LEN,))
        x = ngram_block(n, len(ALPHABET))(inputs)
        x = layers.Embedding(pow(len(ALPHABET), n), n * EMBEDDING_DIM)(x)
        for filters, kernel_size, pool_size in LAYER_PARAMS:
            x = layers.Conv1D(filters, kernel_size, activation="relu")(x)
            x = layers.BatchNormalization()(x)
            x = layers.SpatialDropout1D(0.05+0.1*n)(x)
            x = layers.MaxPooling1D(pool_size)(x)
        x = layers.GlobalAveragePooling1D()(x)
        x = layers.Dense(TOPIC_COUNT, activation="softmax")(x)
        model = models.Model(inputs=inputs, outputs=x)
        
        model.compile(optimizer=optimizers.Adadelta(),
                    loss="categorical_crossentropy", metrics=["acc"])
        return model

    #loops for 1,2- and 3-grams over the model(grams with n>2 use A LOT of Memory!)
    for n in range(1, 2):
        print("################### Start Run with n=",n,"########################")
        if loadModelFromDrive:
            model=models.load_model(model_load_name)
            print("model loaded as '"+model_load_name+"'")
        else:
            model=build_ngram_model(n)
            print("model created")
        model.fit(X, y, epochs=epochs, batch_size=4, validation_split=0.2)
        print("################### End Run with n=",n,"  ########################")

    if saveModelToDrive:
        model.save(model_save_name)    
        print("model saved as: '"+model_save_name+"'")

if __name__=="__main__":
    main()